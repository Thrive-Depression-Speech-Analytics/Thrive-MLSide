{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r0ldWvsZpS2",
        "outputId": "c64a5edd-91d1-48ea-bbf4-99a82985c3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "# Install library requests jika belum terinstal\n",
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive untuk menyimpan data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N4RMCFOZv8j",
        "outputId": "2b441b81-21e5-4350-cf30-ca713c1fa2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "zkNbdBxVZwlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi untuk Membaca Participant_ID"
      ],
      "metadata": {
        "id": "GPRzEuMGZz-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca Participant_ID dari CSV TRAIN\n",
        "def read_participantID_train_from_csv():\n",
        "    df = pd.read_csv('https://dcapswoz.ict.usc.edu/wwwdaicwoz/train_split_Depression_AVEC2017.csv')\n",
        "    participant_ID = df['Participant_ID']\n",
        "    return participant_ID\n",
        "\n",
        "# Fungsi untuk membaca Participant_ID dari CSV TEST\n",
        "def read_participantID_test_from_csv():\n",
        "    df = pd.read_csv('https://dcapswoz.ict.usc.edu/wwwdaicwoz/test_split_Depression_AVEC2017.csv')\n",
        "    participant_ID = df['Participant_ID'].tolist()\n",
        "    return participant_ID\n",
        "\n",
        "# Fungsi untuk membaca Participant_ID dari CSV DEV\n",
        "def read_participantID_dev_from_csv():\n",
        "    df = pd.read_csv('https://dcapswoz.ict.usc.edu/wwwdaicwoz/dev_split_Depression_AVEC2017.csv')\n",
        "    participant_ID = df['Participant_ID'].tolist()\n",
        "    return participant_ID"
      ],
      "metadata": {
        "id": "q0SZTTwocclM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi untuk Membaca PHQ8_Binary"
      ],
      "metadata": {
        "id": "TcuHxE9tdIMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk membaca PHQ8_Binary dari CSV TRAIN\n",
        "def read_PHQ8Train_from_csv(participant_ID):\n",
        "    df = pd.read_csv('https://dcapswoz.ict.usc.edu/wwwdaicwoz/train_split_Depression_AVEC2017.csv')\n",
        "    PHQ8 = df.loc[df['Participant_ID'] == participant_ID, 'PHQ8_Binary'].values[0]\n",
        "    return PHQ8\n",
        "\n",
        "# Fungsi untuk membaca PHQ8_Binary dari CSV DEV\n",
        "def read_PHQ8Dev_from_csv(participant_ID):\n",
        "    df = pd.read_csv('https://dcapswoz.ict.usc.edu/wwwdaicwoz/dev_split_Depression_AVEC2017.csv')\n",
        "    PHQ8 = df.loc[df['Participant_ID'] == participant_ID, 'PHQ8_Binary'].values[0]\n",
        "    return PHQ8"
      ],
      "metadata": {
        "id": "SxHKRZ-scmJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi untuk Memproses Data ZIP"
      ],
      "metadata": {
        "id": "zKvqWoVudTWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mendownload dan mengolah data dari file ZIP berdasarkan participant_ID dan kategori\n",
        "def process_data(participant_ids, base_url, destination_base_folder, read_PHQ8_func):\n",
        "    for participantID in participant_ids:\n",
        "        zip_url = f\"{base_url}{participantID}_P.zip\"\n",
        "        response = requests.get(zip_url)\n",
        "        zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
        "\n",
        "        df_PHQ8 = read_PHQ8_func(participantID)\n",
        "\n",
        "        # Tentukan kategori berdasarkan PHQ-8 (1 = 'depressed', 0 = 'normal')\n",
        "        category = 'depressed' if df_PHQ8 == 1 else 'normal'\n",
        "        print(df_PHQ8)\n",
        "\n",
        "        # Buat folder untuk kategori jika belum ada\n",
        "        destination_folder = os.path.join(destination_base_folder, category)\n",
        "        os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "        # Ekstrak file audio dari file ZIP ke dalam folder yang sesuai\n",
        "        for file_info in zip_file.infolist():\n",
        "            if file_info.filename.endswith('.wav'):\n",
        "                with zip_file.open(file_info) as file:\n",
        "                    with open(os.path.join(destination_folder, os.path.basename(file_info.filename)), 'wb') as output_file:\n",
        "                        shutil.copyfileobj(file, output_file)"
      ],
      "metadata": {
        "id": "GYOkyTrYcqmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proses Data TRAIN"
      ],
      "metadata": {
        "id": "6spOIvAvaFPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant_ids_train = read_participantID_train_from_csv()\n",
        "process_data(participant_ids_train, 'https://dcapswoz.ict.usc.edu/wwwdaicwoz/', os.path.join('drive', 'MyDrive', 'DAIC-WOZ Dataset', 'train'), read_PHQ8Train_from_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gojKMBcoaFdB",
        "outputId": "b145ce7c-6cd3-4574-cb6e-301f2d7e61d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proses Data DEV"
      ],
      "metadata": {
        "id": "9MwArfrOdddR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant_ids_dev = read_participantID_dev_from_csv()\n",
        "process_data(participant_ids_dev, 'https://dcapswoz.ict.usc.edu/wwwdaicwoz/', os.path.join('drive', 'MyDrive', 'DAIC-WOZ Dataset', 'dev'), read_PHQ8Dev_from_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irw-YhbqaHGQ",
        "outputId": "334bc87c-93ea-4b64-eb02-4c6f8d16c1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi Preproses dan Ekstraksi Fitur Audio"
      ],
      "metadata": {
        "id": "L0ItZfzZdhxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Wj1dAUiTaIa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi untuk preprocess audio segments"
      ],
      "metadata": {
        "id": "pgbzTl5sdltQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio_segments(file_path, target_sr, segment_duration, skip_duration):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio = librosa.resample(y=audio, orig_sr=sr, target_sr=target_sr)\n",
        "    segment_length = target_sr * segment_duration\n",
        "    skip_length = target_sr * skip_duration\n",
        "    audio = audio[skip_length:-skip_length]\n",
        "    start_segment = audio[:segment_length]\n",
        "    end_segment = audio[-segment_length:]\n",
        "    processed_audio = np.concatenate((start_segment, end_segment))\n",
        "    return processed_audio, target_sr"
      ],
      "metadata": {
        "id": "VvmYVzR0aJ4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi untuk ekstraksi fitur MFCC dari audio"
      ],
      "metadata": {
        "id": "a5QDrzmJdrBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc_features(audio, sr, n_mfcc=13):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
        "    return mfccs\n",
        "\n",
        "# Bagian: Definisi Parameter dan Folder Path\n",
        "target_sr = 22050\n",
        "segment_duration = 3 * 60\n",
        "skip_duration = 30\n",
        "\n",
        "# Folder paths\n",
        "normal_folder = os.path.join('drive', 'MyDrive', 'DAIC-WOZ Dataset', 'train', 'normal')\n",
        "depressed_folder = os.path.join('drive', 'MyDrive', 'DAIC-WOZ Dataset', 'train', 'depressed')\n",
        "\n",
        "normal_files = [os.path.join(normal_folder, file) for file in os.listdir(normal_folder) if file.endswith('.wav')]\n",
        "depressed_files = [os.path.join(depressed_folder, file) for file in os.listdir(depressed_folder) if file.endswith('.wav')]\n",
        "\n",
        "# Gabungkan semua file\n",
        "all_files = depressed_files + normal_files\n",
        "labels = ['0'] * len(normal_files) + ['1'] * len(depressed_files)\n",
        "\n",
        "# Bagi data ke train dan validation set\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(all_files, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# Array buat nyimpen audio yang udah di resample\n",
        "train_audios = []\n",
        "val_audios = []\n",
        "\n",
        "# Array buat nyimpen fitur audio\n",
        "train_mfcc_features = []\n",
        "val_mfcc_features = []"
      ],
      "metadata": {
        "id": "kzmMECaRaK-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Audio dan Ekstraksi Fitur MFCC"
      ],
      "metadata": {
        "id": "tpdW_OoYdxwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preproses setiap audio file di train set\n",
        "for file in train_files:\n",
        "    audio, sr = preprocess_audio_segments(file, target_sr, segment_duration, skip_duration)\n",
        "    train_audios.append(audio)\n",
        "    mfccs = extract_mfcc_features(audio, sr)\n",
        "    train_mfcc_features.append(mfccs)\n",
        "\n",
        "# Preproses setiap audio file di validation set\n",
        "for file in val_files:\n",
        "    audio, sr = preprocess_audio_segments(file, target_sr, segment_duration, skip_duration)\n",
        "    val_audios.append(audio)\n",
        "    mfccs = extract_mfcc_features(audio, sr)\n",
        "    val_mfcc_features.append(mfccs)\n",
        "\n",
        "# Konversi list ke numpy array\n",
        "train_mfcc_features = np.array(train_mfcc_features)\n",
        "val_mfcc_features = np.array(val_mfcc_features)\n",
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "# Reshape fitur MFCC untuk menjadi input ke CNN\n",
        "train_mfcc_features = train_mfcc_features[..., np.newaxis]\n",
        "val_mfcc_features = val_mfcc_features[..., np.newaxis]"
      ],
      "metadata": {
        "id": "qo6U5y_SaML5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pembangunan Model CNN"
      ],
      "metadata": {
        "id": "kv1s3YUCd2R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library TensorFlow dan Keras untuk membangun model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "RcKyhkHcaNX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bangun model CNN\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(train_mfcc_features.shape[1], train_mfcc_features.shape[2], 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Konversi label ke tipe data integer\n",
        "train_labels = np.array([int(label) for label in train_labels])\n",
        "val_labels = np.array([int(label) for label in val_labels])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print ringkasan model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E78ppDmGaOXB",
        "outputId": "cbf84d2d-45b8-4c1d-a1ad-a20facc854b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 13, 15504, 32)     320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 6, 7752, 32)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 6, 7752, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 3, 3876, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3876, 128)      73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 1, 1938, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 248064)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               31752320  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31845121 (121.48 MB)\n",
            "Trainable params: 31845121 (121.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training dan Evaluasi Model"
      ],
      "metadata": {
        "id": "u9zzocOMeAu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "history = model.fit(\n",
        "    train_mfcc_features,\n",
        "    train_labels,\n",
        "    epochs=50,\n",
        "    batch_size=10,\n",
        "    validation_data=(val_mfcc_features, val_labels)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmdbs0l1aPWi",
        "outputId": "95c2c575-700d-4ade-9374-43fc2d646ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 48s 5s/step - loss: 756.9223 - accuracy: 0.5647 - val_loss: 121.0800 - val_accuracy: 0.7273\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 55.2396 - accuracy: 0.6118 - val_loss: 1.4166 - val_accuracy: 0.7273\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 1.5874 - accuracy: 0.6588 - val_loss: 0.6637 - val_accuracy: 0.7273\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 60s 7s/step - loss: 0.7657 - accuracy: 0.7176 - val_loss: 0.6611 - val_accuracy: 0.7273\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 49s 6s/step - loss: 0.6484 - accuracy: 0.7176 - val_loss: 0.6084 - val_accuracy: 0.7273\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 62s 6s/step - loss: 0.6072 - accuracy: 0.7176 - val_loss: 0.5786 - val_accuracy: 0.7273\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 48s 6s/step - loss: 0.6240 - accuracy: 0.7176 - val_loss: 0.5792 - val_accuracy: 0.7273\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 47s 5s/step - loss: 0.6157 - accuracy: 0.7059 - val_loss: 0.5619 - val_accuracy: 0.7273\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.5603 - accuracy: 0.7176 - val_loss: 0.5776 - val_accuracy: 0.7273\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6055 - accuracy: 0.7176 - val_loss: 0.5598 - val_accuracy: 0.7273\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.7457 - accuracy: 0.7176 - val_loss: 0.6398 - val_accuracy: 0.7273\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6264 - accuracy: 0.7176 - val_loss: 0.5788 - val_accuracy: 0.7273\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 51s 6s/step - loss: 0.6135 - accuracy: 0.7176 - val_loss: 0.5640 - val_accuracy: 0.7273\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5829 - accuracy: 0.7176 - val_loss: 0.5511 - val_accuracy: 0.7273\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.5885 - accuracy: 0.7176 - val_loss: 0.5713 - val_accuracy: 0.7273\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.5658 - accuracy: 0.7176 - val_loss: 0.5282 - val_accuracy: 0.7273\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5759 - accuracy: 0.7176 - val_loss: 0.5642 - val_accuracy: 0.7273\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6142 - accuracy: 0.7176 - val_loss: 0.5996 - val_accuracy: 0.7273\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5941 - accuracy: 0.7176 - val_loss: 0.5154 - val_accuracy: 0.7273\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6584 - accuracy: 0.7176 - val_loss: 0.5497 - val_accuracy: 0.7273\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5641 - accuracy: 0.7176 - val_loss: 0.5488 - val_accuracy: 0.7273\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5657 - accuracy: 0.7176 - val_loss: 0.5585 - val_accuracy: 0.7273\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5749 - accuracy: 0.7176 - val_loss: 0.5546 - val_accuracy: 0.7273\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5787 - accuracy: 0.7176 - val_loss: 0.5623 - val_accuracy: 0.7273\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5329 - accuracy: 0.7529 - val_loss: 0.5403 - val_accuracy: 0.8182\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 49s 6s/step - loss: 0.3355 - accuracy: 0.8471 - val_loss: 0.4804 - val_accuracy: 0.8182\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 48s 5s/step - loss: 0.5689 - accuracy: 0.7412 - val_loss: 0.5763 - val_accuracy: 0.7273\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.5909 - accuracy: 0.7176 - val_loss: 0.5524 - val_accuracy: 0.7273\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5848 - accuracy: 0.7176 - val_loss: 0.5532 - val_accuracy: 0.7273\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5764 - accuracy: 0.7176 - val_loss: 0.5709 - val_accuracy: 0.7273\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.5802 - accuracy: 0.7176 - val_loss: 0.5520 - val_accuracy: 0.7273\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.5536 - accuracy: 0.7176 - val_loss: 0.5410 - val_accuracy: 0.7273\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 48s 5s/step - loss: 0.6683 - accuracy: 0.7176 - val_loss: 0.6380 - val_accuracy: 0.8182\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5951 - accuracy: 0.6588 - val_loss: 0.5298 - val_accuracy: 0.7273\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 42s 5s/step - loss: 0.5868 - accuracy: 0.7176 - val_loss: 0.5236 - val_accuracy: 0.7273\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 47s 5s/step - loss: 0.5546 - accuracy: 0.7294 - val_loss: 0.4864 - val_accuracy: 0.7273\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5299 - accuracy: 0.7412 - val_loss: 0.9187 - val_accuracy: 0.7273\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.6492 - accuracy: 0.7294 - val_loss: 0.6344 - val_accuracy: 0.7273\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.6028 - accuracy: 0.7176 - val_loss: 0.5779 - val_accuracy: 0.7273\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 52s 6s/step - loss: 0.6230 - accuracy: 0.7176 - val_loss: 0.5626 - val_accuracy: 0.7273\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 42s 5s/step - loss: 0.5834 - accuracy: 0.7176 - val_loss: 0.5543 - val_accuracy: 0.7273\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5760 - accuracy: 0.7176 - val_loss: 0.5390 - val_accuracy: 0.7273\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5745 - accuracy: 0.7176 - val_loss: 0.5354 - val_accuracy: 0.7273\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.5823 - accuracy: 0.7176 - val_loss: 0.5342 - val_accuracy: 0.7273\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5694 - accuracy: 0.7176 - val_loss: 0.5366 - val_accuracy: 0.7273\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 56s 6s/step - loss: 0.5670 - accuracy: 0.7412 - val_loss: 0.5352 - val_accuracy: 0.7273\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 46s 5s/step - loss: 0.5719 - accuracy: 0.7412 - val_loss: 0.5280 - val_accuracy: 0.7273\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.5441 - accuracy: 0.7294 - val_loss: 0.5491 - val_accuracy: 0.8182\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 43s 5s/step - loss: 0.5609 - accuracy: 0.7412 - val_loss: 0.5254 - val_accuracy: 0.7273\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 45s 5s/step - loss: 0.5670 - accuracy: 0.7529 - val_loss: 0.5287 - val_accuracy: 0.7273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi model\n",
        "loss, accuracy = model.evaluate(train_mfcc_features, train_labels)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(val_mfcc_features, val_labels)\n",
        "print(f'Validation Loss: {val_loss}')\n",
        "print(f'Validation Accuracy: {val_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-UEf0QBJTPb",
        "outputId": "82b3a72f-92dc-4d9f-8f0f-4fc4be0e28e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 12s 4s/step - loss: 0.5414 - accuracy: 0.7529\n",
            "Loss: 0.5413769483566284\n",
            "Accuracy: 0.7529411911964417\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5287 - accuracy: 0.7273\n",
            "Validation Loss: 0.5287379622459412\n",
            "Validation Accuracy: 0.7272727489471436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menyimpan model dan ubah ke tf lite\n"
      ],
      "metadata": {
        "id": "2sf4GvSOJWna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('thrive_model.h5')\n",
        "print(\"Model has been saved as 'thrive_model.h5'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM8xDJj_Jf5x",
        "outputId": "3e412694-7772-4874-bde7-a3fd108b4c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been saved as 'thrive_model.h5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to TFLite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "bl7SioCUJjhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the TFLite model\n",
        "with open('thrive_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"Model has been converted to TFLite and saved as 'thrive_model.tflite'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1on30Z6JlFC",
        "outputId": "b11d59cc-3caa-46fc-ca29-906d2a80cece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been converted to TFLite and saved as 'thrive_model.tflite'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model .h5 ke Google Drive\n",
        "!cp thrive_model.h5 /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "bZVPPtaQVh2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model .tflite ke Google Drive\n",
        "!cp thrive_model.tflite /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "CrqEg2ntVy0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediksi Menggunakan Model HDF5"
      ],
      "metadata": {
        "id": "s6AFs06UMKK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('thrive_model.h5')\n",
        "print(\"Model loaded from 'thrive_model.h5'.\")\n",
        "\n",
        "# Siapkan data untuk prediksi\n",
        "sample_data = val_mfcc_features[0:1]  # Mengambil satu contoh untuk prediksi\n",
        "\n",
        "# Lakukan prediksi\n",
        "predictions = model.predict(sample_data)\n",
        "print(f'Prediction: {predictions}')\n",
        "\n",
        "# Tentukan ambang batas untuk klasifikasi\n",
        "threshold = 1\n",
        "\n",
        "# Interpretasikan hasil prediksi\n",
        "if predictions[0][0] >= threshold:\n",
        "    result = \"depresi\"\n",
        "    label = 1\n",
        "else:\n",
        "    result = \"normal\"\n",
        "    label = 0\n",
        "\n",
        "print(f'Result: {result}')\n",
        "print(f'Label: {label}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbRj26ApMW_W",
        "outputId": "fddb9e83-b689-43d9-9a64-e7343888e980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from 'thrive_model.h5'.\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "Prediction: [[0.17054704]]\n",
            "Result: normal\n",
            "Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediksi Menggunakan Model TFLite"
      ],
      "metadata": {
        "id": "_g36tLSWMj33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path='thrive_model.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Siapkan data untuk prediksi\n",
        "sample_data = val_mfcc_features[0:1]  # Mengambil satu contoh untuk prediksi\n",
        "\n",
        "# Pastikan data dalam format yang benar (float32)\n",
        "sample_data = np.array(sample_data, dtype=np.float32)\n",
        "\n",
        "# Set input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], sample_data)\n",
        "\n",
        "# Jalankan interpreter\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get output tensor\n",
        "predictions = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Interpretasikan hasil prediksi\n",
        "threshold = 1\n",
        "if predictions[0][0] >= threshold:\n",
        "    result = \"depresi\"\n",
        "    label = 1\n",
        "else:\n",
        "    result = \"normal\"\n",
        "    label = 0\n",
        "\n",
        "print(f'Result: {result}')\n",
        "print(f'Label: {label}')\n",
        "print(f'Prediction: {predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nw7OPvLMmNa",
        "outputId": "5f322f01-dc93-49c7-d430-77b1e4c47695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: normal\n",
            "Label: 0\n",
            "Prediction: [[0.17054707]]\n"
          ]
        }
      ]
    }
  ]
}